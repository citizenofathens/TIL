

## :bulb: 회귀?

머신러닝 지도 학습에서 연속적인 값을 예측하는 것을 회귀(regression)문제라고 한다. 
  
그에 반해 분류(classification) 는 이산적인 값을 예측한다

## 로지스틱 회귀분석에서 쓰이는 sigmoid 함수



## :bulb: Sigmoid 함수

로지스틱 회귀분석 

데이터를 ~~0또는 1로 분류하는~~ 두 개의 그룹으로 분류하는 문제에서 쓰이는 분석기법


데이터를 두 개의 분류로 나누는 데 __로지스틱 회귀 분석에서__ 선형 함수를 사용하면 안되는 이유는


데이터를 선형 함수로 ~~분류를 했을 때~~ 분류 했다고 하자.
  
  
학습 데이터를 분류하기 위하여 


선형 함수 y=mx+b 가 있다고 하자.


~~y값이 0 또는 1 (성공 또는 실패) 치역으로 정해져 있을 때 학습 데이터가 이 값 이상이거나 이하인 것은 크게 의미가 없다.~~
  
  
처음에는 언뜻 잘 분류한 것으로 보일 수 있다. 
  
  
  
허나 함정은 그 분류하는 함수의 기울기와 편향이 분류할 때 정해졌다는 것이다.  


그 함수의 기울기와 편향을 벗어나는 값이 들어간다고 가정해 보자 


그렇다면 선형 함수에 그 데이터를 적용하면 그 데이터에 맞게끔 선형 함수의 기울기와 편향이 조정될 수가 있고 그렇게 되면 


기존의 데이터도 잘 분류하지 못하게 되는 경직된 선형 함수의 허점이 드러난다.
  
 

종속 변수 예측 값이 실수 전체일 때 선형 함수가 의미가 있다.

  
  

단순 선형 회귀 분석은 목표가 실수값 예측이고 목표 예측 변수가 하나이다. 선형함수 를 사용 
 
  
로지스틱 회귀 분석에서는  종속 변수가 0 또는 1 이기 때문에 선형 함수를 사용해서 로지스틱 회귀 분석을 하는 것은 의미가 없다 
  
  
또 정의역이 실수 전체이지만 새로운 학습 데이터가 들어 왔을 때 그 학습 데이터 자체가 선형 함수를 크게 벗어난다고 해도 분류를 잘 할 수 없게 되고


또 그 학습 데이터를 분류하기 위해서 기울기 혹은 b(편향?) 값이 조정 된다면 그 이전 데이터는 잘 학습 할 수 없는 함수 모델이 되게 된다. 
  


  
  
   
이를 극복하기 위하여 곡선 형태의 함수를 사용하고 이 같은 형태의 시그모이드 함수를 사용하는데 


시그모이드 함수가 탄생하는 과정은 

선형 회귀 분석을 극복하기 위해 로지스틕 회귀분석에서 의도한 대로 

확률 p 가 주어졌을 때 

확률 p의 범위는  0,1 이 되고 여기에 odds를 씌워 (odds 개념 모르면 search)

(로지스틱 회귀 분석에서는 예측 값이 0 또는 1이기 때문에 선형회귀분석이 의미가 없다 . 그.래.서 Odds를 사용한다)
  
 
(0,∞) 로 만든다 


그 이후 log를 사용하여 (∞,∞)로 만들어 

log(odds(p)) 의 값이 실수 전체이므로 선형회귀분석이 의미 있게 만들어
 

log(odds(p)) = mx + b 이 식을 만들 수 있게 되고


이 식에 대해 p에 대해 정리하면 시그모이드 식이 되므로 시그모이드 식에 대해 w b 값을 구하면 위의 식의 w b 값을 구하는 것과 마찬가지이다
  
  
  
 
따라서 로지스틱 회귀 문제는 확률이 들어왔을 때
 
 학습 데이터를 잘 설명하는 시그모이드 함수의 w와 b를 찾는 문제라고 할 수있다 
 
 
 
~~정의역이 실수 전체이면서 분류하는 구간의 기울기는 급격하고 0과 1에 가까워 지는 쪽은 기울기가 아주 완만하다면~~
 
~~정의역이 실수 전체인 집합에 대해 어떤 데이터가 1을 아주 초과하면서 x값이 무한대에 가까운 즉 (∞,200) 의 데이터가 온다 하더라도 기울기가 변하지는 않기 때문에 의미 있게 이전 데이터와~~

~~새로 들어온 데이터를 분류 할 수 있는 함수 모델이 된다.~~

  
 ### :question: 취소선은 지극히 ~~무시 해도 되는 글~~ 내가 최초에 생각한 개념 의미가 없지 않다 무시 하지 말자 개념 수정 과정이므로
 
 ## 참조 https://icim.nims.re.kr/post/easyMath/64
